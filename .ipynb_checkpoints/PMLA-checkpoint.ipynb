{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09da753a-f63b-44cb-be9a-de8d6eab8105",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de8db61b-b198-4c7e-90ac-c743d03dd641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1ab2b541-143d-4ee2-b56e-236f9f275e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "data = pd.read_csv(\"dataset/loan_data.csv\")\n",
    "original_data = data.copy()\n",
    "data[\"Default\"] = data[\"Default\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e05d3982-4986-4160-b23f-fd3477091ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA PREVIEW: \n",
      "       LoanID  Age  Income  LoanAmount  CreditScore  MonthsEmployed  \\\n",
      "0  I38PQUQS96   56   85994       50587          520              80   \n",
      "1  HPSK72WA7R   69   50432      124440          458              15   \n",
      "2  C1OZ6DPJ8Y   46   84208      129188          451              26   \n",
      "3  V2KKSFM3UN   32   31713       44799          743               0   \n",
      "4  EY08JDHTZP   60   20437        9139          633               8   \n",
      "\n",
      "   NumCreditLines  InterestRate  LoanTerm  DTIRatio    Education  \\\n",
      "0               4         15.23        36      0.44   Bachelor's   \n",
      "1               1          4.81        60      0.68     Master's   \n",
      "2               3         21.17        24      0.31     Master's   \n",
      "3               3          7.07        24      0.23  High School   \n",
      "4               4          6.51        48      0.73   Bachelor's   \n",
      "\n",
      "  EmploymentType MaritalStatus HasMortgage HasDependents LoanPurpose  \\\n",
      "0      Full-time      Divorced         Yes           Yes       Other   \n",
      "1      Full-time       Married          No            No       Other   \n",
      "2     Unemployed      Divorced         Yes           Yes        Auto   \n",
      "3      Full-time       Married          No            No    Business   \n",
      "4     Unemployed      Divorced          No           Yes        Auto   \n",
      "\n",
      "  HasCoSigner  Default LoanApprovalStatus  \n",
      "0         Yes    False           Approved  \n",
      "1         Yes    False           Approved  \n",
      "2          No     True           Rejected  \n",
      "3          No    False           Rejected  \n",
      "4          No    False           Rejected  \n",
      "\n",
      "MISSING VALUES: \n",
      "LoanID                0\n",
      "Age                   0\n",
      "Income                0\n",
      "LoanAmount            0\n",
      "CreditScore           0\n",
      "MonthsEmployed        0\n",
      "NumCreditLines        0\n",
      "InterestRate          0\n",
      "LoanTerm              0\n",
      "DTIRatio              0\n",
      "Education             0\n",
      "EmploymentType        0\n",
      "MaritalStatus         0\n",
      "HasMortgage           0\n",
      "HasDependents         0\n",
      "LoanPurpose           0\n",
      "HasCoSigner           0\n",
      "Default               0\n",
      "LoanApprovalStatus    0\n",
      "dtype: int64\n",
      "\n",
      "DATATYPES: \n",
      "LoanID                 object\n",
      "Age                     int64\n",
      "Income                  int64\n",
      "LoanAmount              int64\n",
      "CreditScore             int64\n",
      "MonthsEmployed          int64\n",
      "NumCreditLines          int64\n",
      "InterestRate          float64\n",
      "LoanTerm                int64\n",
      "DTIRatio              float64\n",
      "Education              object\n",
      "EmploymentType         object\n",
      "MaritalStatus          object\n",
      "HasMortgage            object\n",
      "HasDependents          object\n",
      "LoanPurpose            object\n",
      "HasCoSigner            object\n",
      "Default                  bool\n",
      "LoanApprovalStatus     object\n",
      "dtype: object\n",
      "\n",
      "CLASS BALANCE: \n",
      "LoanApprovalStatus\n",
      "Rejected    0.679436\n",
      "Approved    0.320564\n",
      "Name: proportion, dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analysing the dataset\n",
    "print(f\"DATA PREVIEW: \\n{data.head()}\\n\")\n",
    "\n",
    "# Handling Missing Values\n",
    "print(f\"MISSING VALUES: \\n{data.isnull().sum()}\\n\")\n",
    "print(f\"DATATYPES: \\n{data.dtypes}\\n\")\n",
    "print(f\"CLASS BALANCE: \\n{data[\"LoanApprovalStatus\"].value_counts(normalize = True)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ecd250-0eee-46e0-bbc1-f91484640a7d",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "da04ba17-00a0-408a-b594-14fc0ddbca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns: ['Education', 'EmploymentType', 'MaritalStatus', 'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner', 'LoanApprovalStatus', 'Default']\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns (excluding numerical ones)\n",
    "cat_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Exclude LoanID from categorical columns before encoding\n",
    "cat_columns = [col for col in cat_columns if col != \"LoanID\"]\n",
    "\n",
    "# Manually add 'Default' as a categorical column\n",
    "cat_columns.append(\"Default\")\n",
    "\n",
    "print(f\"Categorical columns: {cat_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ec97977d-5a2f-4f6b-916f-1c467c4435da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Categorical encoding complete! Updated data preview:\n",
      "       LoanID  Age  Income  LoanAmount  CreditScore  MonthsEmployed  \\\n",
      "0  I38PQUQS96   56   85994       50587          520              80   \n",
      "1  HPSK72WA7R   69   50432      124440          458              15   \n",
      "2  C1OZ6DPJ8Y   46   84208      129188          451              26   \n",
      "3  V2KKSFM3UN   32   31713       44799          743               0   \n",
      "4  EY08JDHTZP   60   20437        9139          633               8   \n",
      "\n",
      "   NumCreditLines  InterestRate  LoanTerm  DTIRatio  ...  \\\n",
      "0               4         15.23        36      0.44  ...   \n",
      "1               1          4.81        60      0.68  ...   \n",
      "2               3         21.17        24      0.31  ...   \n",
      "3               3          7.07        24      0.23  ...   \n",
      "4               4          6.51        48      0.73  ...   \n",
      "\n",
      "   LoanPurpose_Business  LoanPurpose_Education  LoanPurpose_Home  \\\n",
      "0                   0.0                    0.0               0.0   \n",
      "1                   0.0                    0.0               0.0   \n",
      "2                   0.0                    0.0               0.0   \n",
      "3                   1.0                    0.0               0.0   \n",
      "4                   0.0                    0.0               0.0   \n",
      "\n",
      "   LoanPurpose_Other  HasCoSigner_No  HasCoSigner_Yes  \\\n",
      "0                1.0             0.0              1.0   \n",
      "1                1.0             0.0              1.0   \n",
      "2                0.0             1.0              0.0   \n",
      "3                0.0             1.0              0.0   \n",
      "4                0.0             1.0              0.0   \n",
      "\n",
      "   LoanApprovalStatus_Approved  LoanApprovalStatus_Rejected  Default_False  \\\n",
      "0                          1.0                          0.0            1.0   \n",
      "1                          1.0                          0.0            1.0   \n",
      "2                          0.0                          1.0            0.0   \n",
      "3                          0.0                          1.0            1.0   \n",
      "4                          0.0                          1.0            1.0   \n",
      "\n",
      "   Default_True  \n",
      "0           0.0  \n",
      "1           0.0  \n",
      "2           1.0  \n",
      "3           0.0  \n",
      "4           0.0  \n",
      "\n",
      "[5 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check if any categorical columns exist before encoding\n",
    "if not cat_columns:\n",
    "    print(\"âš ï¸ No categorical columns found for encoding!\")\n",
    "else:\n",
    "    # One-Hot Encoding for categorical features\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    encoded_cat_features = encoder.fit_transform(data[cat_columns])\n",
    "\n",
    "    # Convert encoded categorical features into a DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_cat_features, columns=encoder.get_feature_names_out(cat_columns))\n",
    "\n",
    "    # Drop original categorical columns and concatenate encoded data\n",
    "    data = data.drop(columns=cat_columns).reset_index(drop=True)\n",
    "    data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "    print(f\"âœ… Categorical encoding complete! Updated data preview:\\n{data.head()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "16eefc3c-90c1-4b56-a8b1-7d260e0047ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical Columns before scaling: ['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio']\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical columns\n",
    "num_columns = original_data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Ensure only existing numerical columns are used for scaling\n",
    "num_columns = [col for col in num_columns if col in data.columns]\n",
    "\n",
    "print(f\"Numerical Columns before scaling: {num_columns}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2e38d-0a72-402f-9b53-37e156d5cfd7",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c7bf0e18-e992-40ec-8021-412db10f26ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data Split | Train: 178742, Val: 38302, Test: 38303\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ðŸš€ Step 7: Drop LoanID (if it exists)\n",
    "if \"LoanID\" in data.columns:\n",
    "    data = data.drop(columns=[\"LoanID\"])\n",
    "    \n",
    "\n",
    "# ðŸ”¹ Extract Target Column\n",
    "y = data[\"LoanApprovalStatus_Approved\"].astype(int)  # Convert target to numeric\n",
    "X = data.drop(columns=[\"LoanApprovalStatus_Approved\", \"LoanApprovalStatus_Rejected\"], errors='ignore')\n",
    "X = X.drop(columns=[\"LoanID\"], errors='ignore')  # Remove LoanID if it exists\n",
    "\n",
    "# ðŸš€ Split data BEFORE applying SMOTE\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"âœ… Data Split | Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be349b3-3be5-46d7-a436-bc3401239ab7",
   "metadata": {},
   "source": [
    "## SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "affd7a25-a8ea-419f-870f-31aff2f70202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SMOTE Applied | New Train Samples: 206454 (Before: 178742)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ðŸš€ Apply SMOTE to only the training data\n",
    "smote = SMOTE(sampling_strategy=0.7, random_state=42)  # Adjust ratio if needed\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"âœ… SMOTE Applied | New Train Samples: {len(X_train_smote)} (Before: {len(X_train)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "feab4d56-290a-4cc4-95d1-cc40c3d0d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size before SMOTE: 178742 samples\n",
      "Dataset size after SMOTE: 206454 samples\n",
      "ðŸ” New samples added: 27712\n",
      "\n",
      "\n",
      "ðŸ”´ Before SMOTE:\n",
      "                               Count  Percentage\n",
      "LoanApprovalStatus_Approved                    \n",
      "0                            121444    67.94374\n",
      "1                             57298    32.05626\n",
      "\n",
      "âœ… After SMOTE:\n",
      "                               Count  Percentage\n",
      "LoanApprovalStatus_Approved                    \n",
      "0                            121444   58.823757\n",
      "1                             85010   41.176243\n"
     ]
    }
   ],
   "source": [
    "# âœ… Correct Dataset Size Comparison\n",
    "print(f\"Dataset size before SMOTE: {X_train.shape[0]} samples\")\n",
    "print(f\"Dataset size after SMOTE: {X_train_smote.shape[0]} samples\")\n",
    "print(f\"ðŸ” New samples added: {X_train_smote.shape[0] - X_train.shape[0]}\")\n",
    "\n",
    "# âœ… Class Distribution Before and After SMOTE\n",
    "before_smote = pd.DataFrame({\n",
    "    \"Count\": y_train.value_counts(),\n",
    "    \"Percentage\": y_train.value_counts(normalize=True) * 100\n",
    "})\n",
    "\n",
    "after_smote = pd.DataFrame({\n",
    "    \"Count\": y_train_smote.value_counts(),\n",
    "    \"Percentage\": y_train_smote.value_counts(normalize=True) * 100\n",
    "})\n",
    "\n",
    "print(\"\\n\\nðŸ”´ Before SMOTE:\\n\", before_smote)\n",
    "print(\"\\nâœ… After SMOTE:\\n\", after_smote)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54b09b6-8de9-44bc-8870-1ceda8ef3c2c",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "61eee6b2-a446-4777-8dda-762bb95fd545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feature Scaling Applied\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ Initialize StandardScaler and Apply Scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Select numerical columns\n",
    "num_cols = X_train_smote.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Fit scaler on training data & transform\n",
    "X_train_scaled = X_train_smote.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[num_cols] = scaler.fit_transform(X_train_smote[num_cols])\n",
    "X_val_scaled[num_cols] = scaler.transform(X_val[num_cols])\n",
    "X_test_scaled[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "print(\"âœ… Feature Scaling Applied\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63c5f24-0273-4877-a94b-53e27a6fd413",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b6d5b37e-739e-4a3f-91b3-98e5542c5df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Early stopping triggered at 11 iterations.\n",
      "âœ… Model trained with best iteration: 11\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ðŸš€ Check class balance to adjust `scale_pos_weight`\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "scale_pos_weight = counts[0] / counts[1] if counts[1] > 0 else 1  # Prevent division by zero\n",
    "\n",
    "# ðŸš€ Optimized Hyperparameters\n",
    "best_params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"random_state\": 42,\n",
    "    \"max_depth\": 2,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 0.65,\n",
    "    \"reg_lambda\": 100,\n",
    "    \"reg_alpha\": 50,\n",
    "    \"gamma\": 0.5,\n",
    "    \"n_jobs\": 4,\n",
    "    \"scale_pos_weight\": scale_pos_weight\n",
    "}\n",
    "\n",
    "# ðŸš€ Train XGBClassifier (Manually Implementing Early Stopping)\n",
    "xgb_model = xgb.XGBClassifier(**best_params, n_estimators=50)  # âœ… No early stopping argument\n",
    "\n",
    "best_iteration = 0\n",
    "best_score = float(\"-inf\")\n",
    "patience = 20  # Stop if no improvement for 20 rounds\n",
    "wait = 0\n",
    "\n",
    "for i in range(1, 51):  # Train up to 50 boosting rounds\n",
    "    xgb_model.n_estimators = i\n",
    "    xgb_model.fit(X_train_scaled, y_train_smote)  # Train with the updated estimator count\n",
    "    \n",
    "    # Validate the model\n",
    "    y_val_pred = xgb_model.predict(X_val_scaled)\n",
    "    score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Check improvement\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_iteration = i\n",
    "        wait = 0  # Reset patience counter\n",
    "    else:\n",
    "        wait += 1  # Increase patience counter\n",
    "    \n",
    "    if wait >= patience:\n",
    "        print(f\"âœ… Early stopping triggered at {best_iteration} iterations.\")\n",
    "        break  # Stop training\n",
    "\n",
    "# Set the final model to best iteration\n",
    "xgb_model.n_estimators = best_iteration\n",
    "xgb_model.fit(X_train_scaled, y_train_smote)  # Retrain on best iteration\n",
    "\n",
    "print(f\"âœ… Model trained with best iteration: {best_iteration}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323b1ce-d333-43b8-a2ef-6114b1c6c354",
   "metadata": {},
   "source": [
    "## Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "37bf24a8-39df-4c30-b018-51d65bd2bbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model Accuracy on Test Data: 0.9526\n",
      "âœ… Precision: 0.8966\n",
      "âœ… Recall: 0.9634\n",
      "âœ… F1 Score: 0.9288\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [206454, 178742]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# ðŸš€ Cross-validation\u001b[39;00m\n\u001b[0;32m     16\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cross_val_score(xgb_model, X_train_scaled, y_train, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâš¡ Cross-validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(cv_scores)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:719\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    717\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 719\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    720\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    721\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    722\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    723\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    724\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    725\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    726\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    727\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    728\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    729\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    730\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    731\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    732\u001b[0m )\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:351\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate metric(s) by cross-validation and also record fit/score times.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <multimetric_cross_validation>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;124;03m[0.28009951 0.3908844  0.22784907]\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    349\u001b[0m params \u001b[38;5;241m=\u001b[39m _check_params_groups_deprecation(fit_params, params, groups)\n\u001b[1;32m--> 351\u001b[0m X, y \u001b[38;5;241m=\u001b[39m indexable(X, y)\n\u001b[0;32m    353\u001b[0m cv \u001b[38;5;241m=\u001b[39m check_cv(cv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:514\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <3x1 sparse matrix ...>]\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    513\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 514\u001b[0m check_consistent_length(\u001b[38;5;241m*\u001b[39mresult)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:457\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    455\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    460\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [206454, 178742]"
     ]
    }
   ],
   "source": [
    "# ðŸš€ Make Predictions\n",
    "y_pred = (xgb_model.predict(X_test_scaled) > 0.5).astype(int)\n",
    "\n",
    "# ðŸš€ Evaluate Model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"âœ… Model Accuracy on Test Data: {accuracy:.4f}\")\n",
    "print(f\"âœ… Precision: {precision:.4f}\")\n",
    "print(f\"âœ… Recall: {recall:.4f}\")\n",
    "print(f\"âœ… F1 Score: {f1:.4f}\")\n",
    "\n",
    "# ðŸš€ Cross-validation\n",
    "cv = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "\n",
    "print(f\"âš¡ Cross-validation Accuracy: {np.mean(cv_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609f6c5-35c9-423c-b583-80afa583f6b1",
   "metadata": {},
   "source": [
    "## Save & Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a1e22d6-8245-4767-8ac6-6d39ff4f6d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model trained and saved successfully!\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=0.65, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric='aucpr', feature_types=None,\n",
      "              feature_weights=None, gamma=0.5, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=2,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=35,\n",
      "              n_jobs=4, num_parallel_tree=None, ...)\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(xgb_model, \"loan_model.pkl\")\n",
    "print(\"âœ… Model trained and saved successfully!\")\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"âœ… Scaler saved successfully!\")\n",
    "\n",
    "# Load the model and scaler for verification\n",
    "model = joblib.load(\"loan_model.pkl\")\n",
    "scaler = joblib.load(\"scaler.pkl\")\n",
    "\n",
    "print(model)  # Should print XGBClassifier with correct parameters\n",
    "print(scaler)  # Should print StandardScaler() with fitted parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "825e658c-5ad1-45f8-be2c-c436fdce71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Training various models\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# # Define models\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=50, min_samples_leaf=25),\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=50, min_samples_leaf=25),\n",
    "#     \"AdaBoost\": AdaBoostClassifier(),\n",
    "#     \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "# }\n",
    "\n",
    "# # Train and evaluate models\n",
    "# results = {}\n",
    "# for name, model in models.items():\n",
    "#     print(f\"Training {name}...\")\n",
    "#     model.fit(X_train, y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "    \n",
    "#     # Compute metrics\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "#     results[name] = {\n",
    "#         \"Accuracy\": accuracy,\n",
    "#         \"Precision\": precision,\n",
    "#         \"Recall\": recall,\n",
    "#         \"F1 Score\": f1\n",
    "#     }\n",
    "\n",
    "# # Print results\n",
    "# print(\"\\nModel Performance:\")\n",
    "# for model, metrics in results.items():\n",
    "#     print(f\"\\n{model}:\")\n",
    "#     for metric, value in metrics.items():\n",
    "#         print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e909b1cf-4691-4c49-9f0c-7bcc0b15263c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATATYPES: \n",
      "Age                               int64\n",
      "Income                            int64\n",
      "LoanAmount                        int64\n",
      "CreditScore                       int64\n",
      "MonthsEmployed                    int64\n",
      "NumCreditLines                    int64\n",
      "InterestRate                    float64\n",
      "LoanTerm                          int64\n",
      "DTIRatio                        float64\n",
      "Education_Bachelor's            float64\n",
      "Education_High School           float64\n",
      "Education_Master's              float64\n",
      "Education_PhD                   float64\n",
      "EmploymentType_Full-time        float64\n",
      "EmploymentType_Part-time        float64\n",
      "EmploymentType_Self-employed    float64\n",
      "EmploymentType_Unemployed       float64\n",
      "MaritalStatus_Divorced          float64\n",
      "MaritalStatus_Married           float64\n",
      "MaritalStatus_Single            float64\n",
      "HasMortgage_No                  float64\n",
      "HasMortgage_Yes                 float64\n",
      "HasDependents_No                float64\n",
      "HasDependents_Yes               float64\n",
      "LoanPurpose_Auto                float64\n",
      "LoanPurpose_Business            float64\n",
      "LoanPurpose_Education           float64\n",
      "LoanPurpose_Home                float64\n",
      "LoanPurpose_Other               float64\n",
      "HasCoSigner_No                  float64\n",
      "HasCoSigner_Yes                 float64\n",
      "LoanApprovalStatus_Approved     float64\n",
      "LoanApprovalStatus_Rejected     float64\n",
      "Default_False                   float64\n",
      "Default_True                    float64\n",
      "dtype: object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"DATATYPES: \\n{data.dtypes}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b64b531-b8b6-4192-b17d-f7805732e409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed',\n",
      "       'NumCreditLines', 'InterestRate', 'LoanTerm', 'DTIRatio',\n",
      "       'Education_Bachelor's', 'Education_High School', 'Education_Master's',\n",
      "       'Education_PhD', 'EmploymentType_Full-time', 'EmploymentType_Part-time',\n",
      "       'EmploymentType_Self-employed', 'EmploymentType_Unemployed',\n",
      "       'MaritalStatus_Divorced', 'MaritalStatus_Married',\n",
      "       'MaritalStatus_Single', 'HasMortgage_No', 'HasMortgage_Yes',\n",
      "       'HasDependents_No', 'HasDependents_Yes', 'LoanPurpose_Auto',\n",
      "       'LoanPurpose_Business', 'LoanPurpose_Education', 'LoanPurpose_Home',\n",
      "       'LoanPurpose_Other', 'HasCoSigner_No', 'HasCoSigner_Yes',\n",
      "       'Default_False', 'Default_True'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "552baa89-d94a-412b-bdb3-c5c21e36d5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loan Approval Prediction: Approved\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained model\n",
    "with open(\"loan_model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Function to preprocess input\n",
    "def preprocess_input(data_dict):\n",
    "    df = pd.DataFrame([data_dict])  # Convert dictionary to DataFrame\n",
    "\n",
    "    # One-hot encode categorical columns\n",
    "    df_encoded = pd.get_dummies(df, columns=['Education', 'EmploymentType', 'MaritalStatus', \n",
    "                                             'HasMortgage', 'HasDependents', 'LoanPurpose', 'HasCoSigner', 'Default'])\n",
    "\n",
    "    # Expected column order (ensure all required columns exist)\n",
    "    expected_columns = [\n",
    "        'Age', 'Income', 'LoanAmount', 'CreditScore', 'MonthsEmployed', 'NumCreditLines',\n",
    "        'InterestRate', 'LoanTerm', 'DTIRatio',\n",
    "        \"Education_Bachelor's\", 'Education_High School', \"Education_Master's\", 'Education_PhD',\n",
    "        'EmploymentType_Full-time', 'EmploymentType_Part-time', 'EmploymentType_Self-employed', 'EmploymentType_Unemployed',\n",
    "        'MaritalStatus_Divorced', 'MaritalStatus_Married', 'MaritalStatus_Single',\n",
    "        'HasMortgage_No', 'HasMortgage_Yes', 'HasDependents_No', 'HasDependents_Yes',\n",
    "        'LoanPurpose_Auto', 'LoanPurpose_Business', 'LoanPurpose_Education', 'LoanPurpose_Home', 'LoanPurpose_Other',\n",
    "        'HasCoSigner_No', 'HasCoSigner_Yes', 'Default_No', 'Default_Yes'\n",
    "    ]\n",
    "\n",
    "    # Ensure all columns exist (missing columns are set to 0)\n",
    "    for col in expected_columns:\n",
    "        if col not in df_encoded.columns:\n",
    "            df_encoded[col] = 0\n",
    "\n",
    "    # Reorder columns to match training data\n",
    "    df_encoded = df_encoded[expected_columns]\n",
    "    \n",
    "    return df_encoded.values\n",
    "\n",
    "# Predefined \"Strong Approval\" Input\n",
    "strong_applicant = {\n",
    "    \"Age\": 25,                # Good age for approval\n",
    "    \"Income\": 90298,         # High income\n",
    "    \"LoanAmount\": 990448,      # Small loan amount relative to income\n",
    "    \"CreditScore\": 220,       # Excellent credit score\n",
    "    \"MonthsEmployed\": 18,     # Stable employment\n",
    "    \"NumCreditLines\": 2,      # Good credit history\n",
    "    \"InterestRate\": 22.72,      # Low interest rate\n",
    "    \"LoanTerm\": 24,           # Reasonable loan term\n",
    "    \"DTIRatio\": round(90448 / 90298, 2),  # Low debt-to-income ratio (20%)\n",
    "    \n",
    "    \"Education\": \"High School\",  # Higher education level\n",
    "    \"EmploymentType\": \"Unemployed\",  \n",
    "    \"MaritalStatus\": \"Single\",\n",
    "    \"HasMortgage\": \"Yes\",\n",
    "    \"HasDependents\": \"No\",\n",
    "    \"LoanPurpose\": \"Business\",    # Common purpose\n",
    "    \"HasCoSigner\": \"No\",     # Increases approval chances\n",
    "    \"Default\": \"Yes\"           # No default history\n",
    "}\n",
    "\n",
    "# Convert 'Default' field properly\n",
    "strong_applicant[\"Default\"] = \"Default_Yes\" if strong_applicant[\"Default\"].lower() == \"yes\" else \"Default_No\"\n",
    "\n",
    "# Preprocess input\n",
    "features = preprocess_input(strong_applicant)\n",
    "\n",
    "# Make prediction\n",
    "prediction = model.predict(features)[0]\n",
    "\n",
    "# Show result\n",
    "result = \"Approved\" if prediction == 1 else \"Rejected\"\n",
    "print(\"\\nLoan Approval Prediction:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cd1347b7-36a5-4c95-9f72-062ed814a81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LoanApprovalStatus\n",
      "0           Approved\n",
      "1           Approved\n",
      "2           Rejected\n",
      "3           Rejected\n",
      "4           Rejected\n",
      "5           Rejected\n",
      "6           Rejected\n",
      "7           Rejected\n",
      "8           Approved\n",
      "9           Approved\n"
     ]
    }
   ],
   "source": [
    "print(df[[\"LoanApprovalStatus\"]].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b81a41-cb7e-48a1-8803-7b664ad9b27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
